{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using a pipeline for summarization"
      ],
      "metadata": {
        "id": "ZKUQFlhkO9OE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dno_JdmlKDJB"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model pipeline\n",
        "summarizer = pipeline(task='summarization',model=\"cnicu/t5-small-booksum\")\n",
        "\n",
        "long_text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'\n",
        "\n",
        "\n",
        "# Pass the long text to the model\n",
        "output = summarizer(long_text,max_length=50)\n",
        "\n",
        "# Access and print the summarized text\n",
        "print(output[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eXt9g7tNZaj",
        "outputId": "6a367c8f-c710-463f-cdc3-6cd7b223f16f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres on each side. It is the second tallest free-standing structure in France after the Millau Viaduct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The quality of summarized text often depends not only on the nature of the input text, but also on the size of the LLM: lightweight models are faster to load and make predictions, but oftentimes tend to sacrifice performance."
      ],
      "metadata": {
        "id": "7LLQsn0SO0dD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating text"
      ],
      "metadata": {
        "id": "XBXEMIDhPCit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn\\'t overshadow the fantastic experience I had.'\n",
        "# Instantiate the pipeline\n",
        "generator = pipeline(task='text-generation', model=\"gpt2\")\n",
        "\n",
        "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
        "\n",
        "# Complete the prompt\n",
        "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
        "\n",
        "# Complete the model pipeline\n",
        "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id, truncation=True)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8Bl1KGKOeuo",
        "outputId": "26fa12ce-24d8-4798-d3f7-e77da2ac3c42"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer review:\n",
            "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
            "\n",
            "Hotel reponse to the customer:\n",
            "Dear valued customer, I am glad to hear you had a good stay with us. The rooms were comfortable and the amenities were top quality. As you mentioned, we were very proud of our guests. Thank you for your continued service and you will be back!\n",
            "\n",
            "Room service:\n",
            "\n",
            "I booked our room on the 4th of September and it was a very pleasant experience. The bed was spacious, but not too big in fact, so I thought I would go for it. The service was great and I enjoyed my stay there! The only hiccup I noticed was the waiters! Not sure what I would say about those, but I'm so glad they are so welcoming!\n",
            "\n",
            "Room service:\n",
            "\n",
            "I was very satisfied with the overall experience, I would recommend checking out the rooms and staying at the riverview hotel as it is a good venue for staying in the city. They have a great staff who are very accommodating and friendly. I would definitely recommend to anyone looking for a place to stay in the city!\n",
            "\n",
            "Room service:\n",
            "\n",
            "I love the Riverview. Love the views and the amazing river views! I had the pleasure of staying here. The room itself was very comfortable and made the experience feel like a family reunion. However, the staff was very rude and rude and made it difficult for me to stay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good prompt can help generate a better output. Even so, smaller models like gpt2 may not always generate the most sensible output."
      ],
      "metadata": {
        "id": "v2vU4QHwPmTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translating text"
      ],
      "metadata": {
        "id": "n295SWxbPscC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_text = \"Este curso sobre LLMs se est√° poniendo muy interesante\"\n",
        "\n",
        "# Define the pipeline\n",
        "translator = pipeline(task='translation_es_to_en', model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the Spanish text\n",
        "translations = translator(spanish_text, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(translations[0][\"translation_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QNBPXmFPxPe",
        "outputId": "6388b178-3ec5-42be-fed4-e77c2bc30e24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This course on LLMs is getting very interesting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translation inference processes are 'learned' differently depending on the source and target language. Typically a suitable model for A specific translation use case."
      ],
      "metadata": {
        "id": "tYwqWSwBQCjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using extractive model for Question Answering"
      ],
      "metadata": {
        "id": "W2CCw2kkRdgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = \"Who painted the Mona Lisa?\"\n",
        "text = 'The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as the most known, visited, talked about, and sung about work of art in the world. The painting\\'s novel qualities include the subject\\'s enigmatic expression, the monumentality of the composition, and the subtle modeling of forms.'\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "output = qa(question=questions, context=text)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip5oty3cQMp6",
        "outputId": "8bdd7a5b-f504-4278-8f91-71069d18d0fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leonardo da Vinci\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using generative model for Question Answering"
      ],
      "metadata": {
        "id": "wDsKHU7xRnBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "input_text = f\"Context: {text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "output = qa({\"context\": text, \"question\": question}, max_length=150)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enBeOuQiTv46",
        "outputId": "7377b5cc-3f4e-49eb-b6ef-ac0ae2e38d71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leonardo da Vinci\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This GPT model is small, so the extractive version likely performed better. We could try using gpt-3.5-turbo next time."
      ],
      "metadata": {
        "id": "SI7Cnaw5T9ct"
      }
    }
  ]
}